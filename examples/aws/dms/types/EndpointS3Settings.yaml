$token: aws:dms/EndpointS3Settings:EndpointS3Settings
properties:
  addColumnName:
    type: boolean
    description: |
      Whether to add column name information to the .csv output file. Default is `false`.
    language:
      python:
        mapCase: false
  bucketFolder:
    type: string
    description: |
      S3 object prefix.
    language:
      python:
        mapCase: false
  bucketName:
    type: string
    description: |
      S3 bucket name.
    language:
      python:
        mapCase: false
  cannedAclForObjects:
    type: string
    description: |
      Predefined (canned) access control list for objects created in an S3 bucket. Valid values include `NONE`, `PRIVATE`, `PUBLIC_READ`, `PUBLIC_READ_WRITE`, `AUTHENTICATED_READ`, `AWS_EXEC_READ`, `BUCKET_OWNER_READ`, and `BUCKET_OWNER_FULL_CONTROL`. Default is `NONE`.
    language:
      python:
        mapCase: false
  cdcInsertsAndUpdates:
    type: boolean
    description: |
      Whether to write insert and update operations to .csv or .parquet output files. Default is `false`.
    language:
      python:
        mapCase: false
  cdcInsertsOnly:
    type: boolean
    description: |
      Whether to write insert operations to .csv or .parquet output files. Default is `false`.
    language:
      python:
        mapCase: false
  cdcMaxBatchInterval:
    type: integer
    description: |
      Maximum length of the interval, defined in seconds, after which to output a file to Amazon S3. Default is `60`.
    language:
      python:
        mapCase: false
  cdcMinFileSize:
    type: integer
    description: |
      Minimum file size, defined in megabytes, to reach for a file output. Default is `32`.
    language:
      python:
        mapCase: false
  cdcPath:
    type: string
    description: |
      Folder path of CDC files. For an S3 source, this setting is required if a task captures change data; otherwise, it's optional. If `cdc_path` is set, AWS DMS reads CDC files from this path and replicates the data changes to the target endpoint. Supported in AWS DMS versions 3.4.2 and later.
    language:
      python:
        mapCase: false
  compressionType:
    type: string
    description: |
      Set to compress target files. Default is `NONE`. Valid values are `GZIP` and `NONE`.
    language:
      python:
        mapCase: false
  csvDelimiter:
    type: string
    description: |
      Delimiter used to separate columns in the source files. Default is `,`.
    language:
      python:
        mapCase: false
  csvNoSupValue:
    type: string
    description: |
      String to use for all columns not included in the supplemental log.
    language:
      python:
        mapCase: false
  csvNullValue:
    type: string
    description: |
      String to as null when writing to the target.
    language:
      python:
        mapCase: false
  csvRowDelimiter:
    type: string
    description: |
      Delimiter used to separate rows in the source files. Default is `\n`.
    language:
      python:
        mapCase: false
  dataFormat:
    type: string
    description: |
      Output format for the files that AWS DMS uses to create S3 objects. Valid values are `csv` and `parquet`. Default is `csv`.
    language:
      python:
        mapCase: false
  dataPageSize:
    type: integer
    description: |
      Size of one data page in bytes. Default is `1048576` (1 MiB).
    language:
      python:
        mapCase: false
  datePartitionDelimiter:
    type: string
    description: |
      Date separating delimiter to use during folder partitioning. Valid values are `SLASH`, `UNDERSCORE`, `DASH`, and `NONE`. Default is `SLASH`.
    language:
      python:
        mapCase: false
  datePartitionEnabled:
    type: boolean
    description: |
      Partition S3 bucket folders based on transaction commit dates. Default is `false`.
    language:
      python:
        mapCase: false
  datePartitionSequence:
    type: string
    description: |
      Date format to use during folder partitioning. Use this parameter when `date_partition_enabled` is set to true. Valid values are `YYYYMMDD`, `YYYYMMDDHH`, `YYYYMM`, `MMYYYYDD`, and `DDMMYYYY`. Default is `YYYYMMDD`.
    language:
      python:
        mapCase: false
  dictPageSizeLimit:
    type: integer
    description: |
      Maximum size in bytes of an encoded dictionary page of a column. Default is `1048576` (1 MiB).
    language:
      python:
        mapCase: false
  enableStatistics:
    type: boolean
    description: |
      Whether to enable statistics for Parquet pages and row groups. Default is `true`.
    language:
      python:
        mapCase: false
  encodingType:
    type: string
    description: |
      Type of encoding to use. Value values are `rle_dictionary`, `plain`, and `plain_dictionary`. Default is `rle_dictionary`.
    language:
      python:
        mapCase: false
  encryptionMode:
    type: string
    description: |
      Server-side encryption mode that you want to encrypt your .csv or .parquet object files copied to S3. Valid values are `SSE_S3` and `SSE_KMS`. Default is `SSE_S3`.
    language:
      python:
        mapCase: false
  externalTableDefinition:
    type: string
    description: |
      JSON document that describes how AWS DMS should interpret the data.
    language:
      python:
        mapCase: false
  ignoreHeadersRow:
    type: integer
    description: |
      When this value is set to `1`, DMS ignores the first row header in a .csv file. Default is `0`.
    language:
      python:
        mapCase: false
  includeOpForFullLoad:
    type: boolean
    description: |
      Whether to enable a full load to write INSERT operations to the .csv output files only to indicate how the rows were added to the source database. Default is `false`.
    language:
      python:
        mapCase: false
  maxFileSize:
    type: integer
    description: |
      Maximum size (in KB) of any .csv file to be created while migrating to an S3 target during full load. Valid values are from `1` to `1048576`. Default is `1048576` (1 GB).
    language:
      python:
        mapCase: false
  parquetTimestampInMillisecond:
    type: boolean
    description: |
      - Specifies the precision of any TIMESTAMP column values written to an S3 object file in .parquet format. Default is `false`.
    language:
      python:
        mapCase: false
  parquetVersion:
    type: string
    description: |
      Version of the .parquet file format. Default is `parquet-1-0`. Valid values are `parquet-1-0` and `parquet-2-0`.
    language:
      python:
        mapCase: false
  preserveTransactions:
    type: boolean
    description: |
      Whether DMS saves the transaction order for a CDC load on the S3 target specified by `cdc_path`. Default is `false`.
    language:
      python:
        mapCase: false
  rfc4180:
    type: boolean
    description: |
      For an S3 source, whether each leading double quotation mark has to be followed by an ending double quotation mark. Default is `true`.
    language:
      python:
        mapCase: false
  rowGroupLength:
    type: integer
    description: |
      Number of rows in a row group. Default is `10000`.
    language:
      python:
        mapCase: false
  serverSideEncryptionKmsKeyId:
    type: string
    description: |
      If you set encryptionMode to `SSE_KMS`, set this parameter to the ARN for the AWS KMS key.
    language:
      python:
        mapCase: false
  serviceAccessRoleArn:
    type: string
    description: |
      ARN of the IAM Role with permissions to read from or write to the S3 Bucket.
    language:
      python:
        mapCase: false
  timestampColumnName:
    type: string
    description: |
      Column to add with timestamp information to the endpoint data for an Amazon S3 target.
    language:
      python:
        mapCase: false
  useCsvNoSupValue:
    type: boolean
    description: |
      Whether to use `csv_no_sup_value` for columns not included in the supplemental log.
    language:
      python:
        mapCase: false
type: object
